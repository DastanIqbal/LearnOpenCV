{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,dlib\n",
    "import numpy as np\n",
    "import math, sys\n",
    "from dataPath import DATA_PATH\n",
    "from dataPath import MODEL_PATH\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTOR_PATH = MODEL_PATH + \"shape_predictor_68_face_landmarks.dat\"\n",
    "RESIZE_HEIGHT = 480\n",
    "NUM_FRAMES_FOR_FPS = 100\n",
    "SKIP_FRAMES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the intereye distance.\n",
    "def interEyeDistance(predict):\n",
    "    leftEyeLeftCorner = (predict[36].x, predict[36].y)\n",
    "    rightEyeLeftCorner = (predict[45].x, predict[45].y)\n",
    "    distance = cv2.norm(np.array(rightEyeLeftCorner) - np.array(leftEyeLeftCorner))\n",
    "    distance = int(distance)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "winName = \"Stabilized facial landmark detector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFileName = 0\n",
    "\n",
    "# Initializing video capture object.\n",
    "cap = cv2.VideoCapture(videoFileName)\n",
    "\n",
    "if(cap.isOpened()==False):\n",
    "  print(\"Unable to load video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "winSize = 101\n",
    "maxLevel = 10\n",
    "fps = 30.0\n",
    "# Grab a frame\n",
    "ret,imPrev = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to grayscale.\n",
    "imGrayPrev = cv2.cvtColor(imPrev, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the size of the image.\n",
    "size = imPrev.shape[0:1]\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "landmarkDetector = dlib.shape_predictor(PREDICTOR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the parameters\n",
    "points=[]\n",
    "pointsPrev=[]\n",
    "pointsDetectedCur=[]\n",
    "pointsDetectedPrev=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyeDistanceNotCalculated = True\n",
    "eyeDistance = 0\n",
    "isFirstFrame = True\n",
    "# Initial value, actual value calculated after 100 frames\n",
    "fps = 10\n",
    "showStabilized = False\n",
    "count =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face detected\n",
      "313\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 14:00:39.268 Python[9200:61514] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-11-06 14:00:39.268 Python[9200:61514] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n",
      "face detected\n",
      "313\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Skipping the frames for faster processing\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (count \u001b[38;5;241m%\u001b[39m SKIP_FRAMES \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m   faces \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimSmallDlib\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# If no face was detected\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  if (count==0):\n",
    "    t = cv2.getTickCount()\n",
    "\n",
    "  # Grab a frame\n",
    "  ret,im = cap.read()\n",
    "  imDlib = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "  # COnverting to grayscale\n",
    "  imGray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "  height = im.shape[0]\n",
    "  IMAGE_RESIZE = float(height)/RESIZE_HEIGHT\n",
    "  # Resize image for faster face detection\n",
    "  imSmall = cv2.resize(im, None, fx=1.0/IMAGE_RESIZE, fy=1.0/IMAGE_RESIZE,interpolation = cv2.INTER_LINEAR)\n",
    "  imSmallDlib = cv2.cvtColor(imSmall, cv2.COLOR_BGR2RGB)\n",
    "  # Skipping the frames for faster processing\n",
    "  if (count % SKIP_FRAMES == 0):\n",
    "    faces = detector(imSmallDlib,0)\n",
    "\n",
    "  # If no face was detected\n",
    "  if len(faces)==0:\n",
    "    print(\"No face detected\")\n",
    "\n",
    "  # If faces are detected, iterate through each image and detect landmark points\n",
    "  else:\n",
    "    for i in range(0,len(faces)):\n",
    "      print(\"face detected\")\n",
    "      # Face detector was found over a smaller image.\n",
    "      # So, we scale face rectangle to correct size.\n",
    "      newRect = dlib.rectangle(int(faces[i].left() * IMAGE_RESIZE),\n",
    "        int(faces[i].top() * IMAGE_RESIZE),\n",
    "        int(faces[i].right() * IMAGE_RESIZE),\n",
    "        int(faces[i].bottom() * IMAGE_RESIZE))\n",
    "      \n",
    "      # Detect landmarks in current frame\n",
    "      landmarks = landmarkDetector(imDlib, newRect).parts()\n",
    "      \n",
    "      # Handling the first frame of video differently,for the first frame copy the current frame points\n",
    "      \n",
    "      if (isFirstFrame==True):\n",
    "        pointsPrev=[]\n",
    "        pointsDetectedPrev = []\n",
    "        [pointsPrev.append((p.x, p.y)) for p in landmarks]\n",
    "        [pointsDetectedPrev.append((p.x, p.y)) for p in landmarks]\n",
    "\n",
    "      # If not the first frame, copy points from previous frame.\n",
    "      else:\n",
    "        pointsPrev=[]\n",
    "        pointsDetectedPrev = []\n",
    "        pointsPrev = points\n",
    "        pointsDetectedPrev = pointsDetectedCur\n",
    "\n",
    "      # pointsDetectedCur stores results returned by the facial landmark detector\n",
    "      # points stores the stabilized landmark points\n",
    "      points = []\n",
    "      pointsDetectedCur = []\n",
    "      [points.append((p.x, p.y)) for p in landmarks]\n",
    "      [pointsDetectedCur.append((p.x, p.y)) for p in landmarks]\n",
    "\n",
    "      # Convert to numpy float array\n",
    "      pointsArr = np.array(points,np.float32)\n",
    "      pointsPrevArr = np.array(pointsPrev,np.float32)\n",
    "\n",
    "      # If eye distance is not calculated before\n",
    "      if eyeDistanceNotCalculated:\n",
    "        eyeDistance = interEyeDistance(landmarks)\n",
    "        print(eyeDistance)\n",
    "        eyeDistanceNotCalculated = False\n",
    "\n",
    "      if eyeDistance > 100:\n",
    "          dotRadius = 3\n",
    "      else:\n",
    "        dotRadius = 2\n",
    "\n",
    "      print(eyeDistance)\n",
    "      sigma = eyeDistance * eyeDistance / 400\n",
    "      s = 2*int(eyeDistance/4)+1\n",
    "\n",
    "      #  Set up optical flow params\n",
    "      lk_params = dict(winSize  = (s, s), maxLevel = 5, criteria = (cv2.TERM_CRITERIA_COUNT | cv2.TERM_CRITERIA_EPS, 20, 0.03))\n",
    "      # Python Bug. Calculating pyramids and then calculating optical flow results in an error. So directly images are used.\n",
    "      # ret, imGrayPyr= cv2.buildOpticalFlowPyramid(imGray, (winSize,winSize), maxLevel)\n",
    "\n",
    "      pointsArr,status, err = cv2.calcOpticalFlowPyrLK(imGrayPrev,imGray,pointsPrevArr,pointsArr,**lk_params)\n",
    "      \n",
    "\n",
    "      # Converting to float\n",
    "      pointsArrFloat = np.array(pointsArr,np.float32)\n",
    "\n",
    "      # Converting back to list\n",
    "      points = pointsArrFloat.tolist()\n",
    "\n",
    "      # Final landmark points are a weighted average of\n",
    "      # detected landmarks and tracked landmarks\n",
    "      for k in range(0,len(landmarks)):\n",
    "        d = cv2.norm(np.array(pointsDetectedPrev[k]) - np.array(pointsDetectedCur[k]))\n",
    "        alpha = math.exp(-d*d/sigma)\n",
    "        points[k] = (1 - alpha) * np.array(pointsDetectedCur[k]) + alpha * np.array(points[k])\n",
    "\n",
    "      # Drawing over the stabilized landmark points\n",
    "      if showStabilized is True:\n",
    "        for p in points:\n",
    "          cv2.circle(im,(int(p[0]),int(p[1])),dotRadius, (255,0,0),-1)\n",
    "      else:\n",
    "        for p in pointsDetectedCur:\n",
    "          cv2.circle(im,(int(p[0]),int(p[1])),dotRadius, (0,0,255),-1)\n",
    "\n",
    "      isFirstFrame = False\n",
    "      count = count+1\n",
    "\n",
    "      # Calculating the fps value\n",
    "      if ( count == NUM_FRAMES_FOR_FPS):\n",
    "        t = (cv2.getTickCount()-t)/cv2.getTickFrequency()\n",
    "        fps = NUM_FRAMES_FOR_FPS/t\n",
    "        count = 0\n",
    "        isFirstFrame = True\n",
    "\n",
    "      # Display the landmarks points\n",
    "      cv2.putText(im, \"{:.1f}-fps\".format(fps), (50, size[0]-50), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0, 0, 255), 3,cv2.LINE_AA)\n",
    "      cv2.imshow(winName, im)\n",
    "      key = cv2.waitKey(25) & 0xFF\n",
    "\n",
    "      # Use spacebar to toggle between Stabilized and Unstabilized version.\n",
    "      if key==32:\n",
    "        showStabilized = not showStabilized\n",
    "\n",
    "      # Stop the program.\n",
    "      if key==27:\n",
    "        sys.exit()\n",
    "      # Getting ready for next frame\n",
    "      imPrev = im\n",
    "      imGrayPrev = imGray\n",
    "\n",
    "cv2.destroyAllwindows()\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
