{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Object Detection using Haar Cascades</font>\n",
    "\n",
    "\n",
    "# <font style=\"color:rgb(50,120,229)\">Overview</font>\n",
    "\n",
    "In the previous weeks, we discussed Object detection based on HOG features. In this module, we will take the example of Face detection to explain Object detection. The same concepts apply to general object detection too. \n",
    "\n",
    "Let’s go back a few years to 2001, when face detection was more of a research topic and many methods were proposed which didn’t work very well. Paul Viola and Michael Jones came up with their seminal [paper](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=G2-nFaIAAAAJ&citation_for_view=G2-nFaIAAAAJ:u5HHmVD_uO8C) which not only detected faces robustly, but did so in real-time. It is one of the most cited papers in Computer Vision. It is worth discussing their approach as they proposed some new ideas through this paper.\n",
    "\n",
    "In the following sections, we will learn how to use haar cascade based face and smile detectors in OpenCV. Then we will cover the theory of Face detection in brief. Finally, we will learn how to train your own haar cascade based object detector in OpenCV. We will learn to train an eye detector. \n",
    "\n",
    "# <font style=\"color:rgb(50,120,229)\">Face Detection in OpenCV</font>\n",
    "\n",
    "In this section, we will see a demo of the face detection implementation using Haar cascades in OpenCV. We will also see a smile detection demo and how to use both the cascades for detecting face and smile in an image. The trained model for face detection and smile detection is present in the models folder, which is downloaded from [here](https://github.com/opencv/opencv/tree/master/data/haarcascades). \n",
    "\n",
    "The face detector is contributed by [Prof. Dr. Rainer Leinhart](http://www.multimedia-computing.de/wiki/Prof._Dr._Rainer_Lienhart) and the smile detector by [Deniz Suarez](http://visilab.etsii.uclm.es/personas/oscar/oscar.html). The models are kept in xml files. There are other haar cascade based object detectors in OpenCV which are trained by various researchers and shared through OpenCV such as eye detector, profile face detector, cat face detector, pedestrian detector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function used in the demo is that of `detectMultiscale`\n",
    "\n",
    "### <font style=\"color:rgb(8,133,37)\">Function Syntax</font>\n",
    "\n",
    "```python\n",
    "cv2.CascadeClassifier.detectMultiScale(image[, scaleFactor[, minNeighbors]])\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- **`image`** is the input grayscale image.\n",
    "- **`objects`** is the rectangular region enclosing the objects detected\n",
    "- **`scaleFactor`** is the parameter specifying how much the image size is reduced at each image scale. It is used to create the scale pyramid. \n",
    "- **`minNeighbors`** is a parameter specifying how many neighbors each candidate rectangle should have, to retain it. Higher number gives lower false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Face Detection</font>\n",
    "\n",
    "In this example, we discuss the effect of the parameters. The effect of `scaleFactor` is mostly related to speed. Lower the value, slower will be the speed. In this example, we check the effect of the `minNeighbors` parameter. As the value of this variable is increased, false positives are decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataPath import DATA_PATH\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (6.0,6.0)\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the cascade classifier from the xml file.\n",
    "faceCascade = cv2.CascadeClassifier(DATA_PATH + 'models/haarcascade_frontalface_default.xml')\n",
    "faceNeighborsMax = 10\n",
    "neighborStep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the image\n",
    "frame = cv2.imread(DATA_PATH + \"images/hillary_clinton.jpg\")\n",
    "frameGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-9ugsgs06/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/nj/yfdjzlhj4ng_ndb0wp1g5rcm0000gn/T/ipykernel_90628/4293025340.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mcount\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mneigh\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfaceNeighborsMax\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mneighborStep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mfaces\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfaceCascade\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetectMultiScale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframeGray\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1.2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mneigh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m     \u001B[0mframeClone\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mframe\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfaces\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31merror\u001B[0m: OpenCV(4.4.0) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-9ugsgs06/opencv/modules/objdetect/src/cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'detectMultiScale'\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1296x1296 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform multi scale detection of faces\n",
    "plt.figure(figsize=(18,18))\n",
    "count = 1\n",
    "for neigh in range(1, faceNeighborsMax, neighborStep):\n",
    "    faces = faceCascade.detectMultiScale(frameGray, 1.2, neigh)\n",
    "    frameClone = np.copy(frame)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frameClone, (x, y), \n",
    "                      (x + w, y + h), \n",
    "                      (255, 0, 0),2)\n",
    "\n",
    "    cv2.putText(frameClone, \n",
    "    \"# Neighbors = {}\".format(neigh), (10, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 6)\n",
    "    \n",
    "    plt.subplot(3,3,count)\n",
    "    plt.imshow(frameClone[:,:,::-1])\n",
    "    count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(50,120,229)\">Face and Smile Detection</font>\n",
    "\n",
    "The effect of minNeighbors was not very pronounced in the face detection example as the face has very unique features. The mouth or smile on the other hand is very difficult to detect without false positives. This is illustrated using the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Detect the face using the cascade\n",
    "faceCascade = cv2.CascadeClassifier(DATA_PATH + 'models/haarcascade_frontalface_default.xml')\n",
    "smileCascade = cv2.CascadeClassifier(DATA_PATH + 'models/haarcascade_smile.xml')\n",
    "smileNeighborsMax = 90\n",
    "neighborStep = 10\n",
    "\n",
    "frame = cv2.imread(DATA_PATH + \"images/hillary_clinton.jpg\")\n",
    "\n",
    "frameGray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(frameGray, 1.4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the face area from the detected face rectangle\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(frame, (x, y), \n",
    "                  (x + w, y + h), \n",
    "                  (255, 0, 0), 2)\n",
    "    faceRoiGray = frameGray[y: y + h, x: x + w]\n",
    "    faceRoiOriginal = frame[y: y + h, x: x + w]\n",
    "\n",
    "count = 1\n",
    "plt.figure(figsize=(18,18))\n",
    "# Detect the smile from the detected face area and display the image\n",
    "for neigh in range(1, smileNeighborsMax, neighborStep):\n",
    "    smile = smileCascade.detectMultiScale(faceRoiGray, \n",
    "                          1.5, neigh)\n",
    "\n",
    "    frameClone = np.copy(frame)\n",
    "    faceRoiClone = frameClone[y: y + h, x: x + w]\n",
    "    for (xx, yy, ww, hh) in smile:\n",
    "        cv2.rectangle(faceRoiClone, (xx, yy), \n",
    "                      (xx + ww, yy + hh), \n",
    "                      (0, 255, 0), 2)\n",
    "\n",
    "    cv2.putText(frameClone, \n",
    "              \"# Neighbors = {}\".format(neigh), \n",
    "              (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "              (0, 0, 255), 4)\n",
    "    plt.subplot(3,3,count)\n",
    "    plt.imshow(frameClone[:,:,::-1])\n",
    "    count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Face Detection Theory</font>\n",
    "\n",
    "The image is divided into sub windows and multiple Haar-like features are computed at different scales and positions for each sub window. The important features are selected using the Adaboost algorithm. Then each sub window is checked for the presence or absence of face using a cascade of classifiers. The detection algorithm uses a cascade of classifiers which use Haar-like features. Thus, it is also called haar cascades based detector.\n",
    "\n",
    "## <font style=\"color:rgb(50,120,229)\">What are Haar Features?</font>\n",
    "\n",
    "\n",
    "\n",
    "|  <center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-harrFeatureexapmle.png\"/></center> |  <center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-haarFeatureVariousSclae.png\"/></center> |\n",
    "| -------- | -------- |\n",
    "| <center>Example Haar features</center>     | <center>Features at various scale</center>    |\n",
    "\n",
    "&nbsp;\n",
    "Haar features are rectangular elements inspired from [Haar wavelets](https://en.wikipedia.org/wiki/Haar_wavelet). The value of a feature is obtained by subtracting the sum of all pixels in the white region from that of the black region. \n",
    "\n",
    "$$\n",
    "f = \\Sigma(\\text{ pixels in black area }) − \\Sigma( \\text{ pixels in white area })\n",
    "$$\n",
    "\n",
    "Some examples are shown in the diagram above. The intuition is that these features can detect edges, lines and other patterns in an image. These features are computed at different scales and different locations as shown above. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-relevantHaarFeature3.png\" width=700/></center></th>\n",
    "        <th><center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-relevantHaarFeature1.png\" width=700/></center></th>\n",
    "        <th><center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-irrelevantHaarFeature.png\" width=700/></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"2\"><center>Relevant Haar Features</center></td>\n",
    "        <td><center>Irrelevant Feature</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "In the figure shown above, when feature **A** is computed at the location shown in the first image, it will give a high value as the bridge is lighter than the nearby area. But if it is computed at some other scale or location as shown in the third image, the feature will give much lower value. Thus, it is a good feature for detecting features that resemble the nose bridge. \n",
    "\n",
    "Similarly, when feature **B** is computed as shown in the second image, it can detect the eye region since the eye region is darker as compared to the region below it. Features **C** and **D** can be used to detect other features and more complex patterns. \n",
    "\n",
    "It should be noted that only a single feature is not capable of detecting faces with high accuracy. But, when many such features vote for the presence or absence of face, the detection becomes very accurate and robust.\n",
    "\n",
    "For a 24 x 24 window used for detecting faces,  we need to compute the feature value at different scales and positions which amounts to ~160,000 features. It will be computationally expensive to find all the features even when the feature computation involves simple operations like addition and subtraction. To tackle this problem, the authors used Integral Image; an idea borrowed from previous work in computer vision and graphics. \n",
    "\n",
    "## <font style=\"color:rgb(50,120,229)\">What is an Integral Image?</font>\n",
    "\n",
    "It is a [summed-area table](https://en.wikipedia.org/wiki/Summed-area_table) whose value at a pixel ( x, y ) is the sum of all pixel values above and to the left of that pixel. As we shall see later, once the integral image is computed, the computation of haar features can be done efficiently in constant time.\n",
    "\n",
    "<center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-image1.png\"/></center>\n",
    "\n",
    "For the pixel ( x, y ) in the Image $I$ shown, the Integral Image $II$ is given by\n",
    "\n",
    "$$\n",
    "II( x, y ) = \\sum_{\\bf{i \\le x , j \\le y}} I( i, j )\n",
    "$$\n",
    "\n",
    "which is the sum of all pixels in the shaded region ( above and left of the pixel ( x, y ). The above formula can be written in recursive format and can be computed efficiently using the formula :\n",
    "\n",
    "$$\n",
    "II( x, y ) = I(x,y) + II(x,y−1) + II(x−1,y) − II(x−1,y−1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The important thing to note is that we can compute the Integral Image ( $II$ ) in a single pass using the above formula. We can use it to calculate the sum of pixel values of any rectangular block in the image, using just 3 arithmetic operations ( 1 addition and 2 subtractions ). \n",
    "\n",
    "Suppose, we want to find the sum of all pixels in the shaded rectangular region <u>abcd</u> for the image given below, the required sum using Integral Image $II$ is given by : \n",
    "\n",
    "$$\n",
    "\\sum_{x,y \\in abcd} I(x,y) = II(a) + II(d) − II(c) − II(b)\n",
    "$$\n",
    "\n",
    "The image below illustrates the above formula. \n",
    "\n",
    "<center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-image2.png\"/></center>\n",
    "\n",
    "## <font style=\"color:rgb(50,120,229)\">Feature Selection using Adaboost</font>\n",
    "\n",
    "As mentioned earlier, not all features are useful for a classification task. Therefore, a feature selection algorithm is proposed so that only a subset of features need to be computed at test time. These special features are selected using the Adaboost algorithm. \n",
    "\n",
    "The training algorithm selects the best features and learns the classifier simultaneously. It uses the notion of weak classifiers, which perform slightly better than random guessing. Let $h_i$ be a weak classifier, which is given by :\n",
    "\n",
    "$$\n",
    "h_{i}({\\mathbf {x}})={\\begin{cases} −1&{\\text{if }}f_{i}<\\theta _{i}\\\\ 1&{\\text{otherwise}}\\end{cases}}\n",
    "$$\n",
    "\n",
    "Where, $x$ is the image sub window under consideration, $f_i$ is the computed feature, $\\theta_i$ is the threshold which is learned during the training process.\n",
    "\n",
    "Multiple weak classifiers are combined to build a strong classifier. The strong classifier is represented as a linear combination of weak classifiers.\n",
    "\n",
    "$$\n",
    "\\bf{h(x)} = \\bf{sign}(\\sum_i (\\alpha_i h_i(x))\n",
    "$$  \n",
    "\n",
    "where, $h(x)$ is the strong classifier and $\\alpha_i$ are the weights which indicate the contribution of each weak classifier towards the final classifier. They are learnt during training.\n",
    "\n",
    "## <font style=\"color:rgb(50,120,229)\">Cascade of Classifiers</font>\n",
    "\n",
    "This is the final stage of the pipeline. The sub windows are passed through a cascade of classifiers which detects whether a face is present or not. The functional block diagram of the same is given below.\n",
    "\n",
    "<center><img src=\"https://www.learnopencv.com/wp-content/uploads/2018/01/opcv4face-w7-m4-blockDiagram.png\"/></center>\n",
    "\n",
    "The idea behind using this cascade of classifiers is that most of the sub windows in an image will not contain the face. So, why waste resources in evaluating all features in that sub window. We want to evaluate the features of only those sub windows which are the most promising candidates for having a face. \n",
    "\n",
    "The first classifier uses only one feature. It rejects those sub windows which do not contain faces with high accuracy. The sub windows which are not rejected by the initial classifiers are evaluated by the classifier in the next stages. As we move forward in the cascade, the number of features evaluated increases and thus the complexity of the classifier also increases. This also makes the later stages more accurate in detecting faces. The face detection cascade used in the paper had 38 stages and more than 6000 features.\n",
    "\n",
    "For example, the first stage uses only one feature, if a face is present, it detects them almost 100% of the time with a false positive rate of 50%. Similarly, the second stage uses 10 features and has a FPR of 20% and so on. We get the final output ( face or no face ) from the last stage which has a very low error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">References and Further Reading</font>\n",
    "\n",
    "1. [https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)\n",
    "\n",
    "2. [http://www.merl.com/publications/docs/TR2004-043.pdf](http://www.merl.com/publications/docs/TR2004-043.pdf)\n",
    "\n",
    "3. [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "4. [http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html)\n",
    "\n",
    "5. [http://docs.opencv.org/3.2.0/db/d28/tutorial_cascade_classifier.html](http://docs.opencv.org/3.2.0/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "6. [http://docs.opencv.org/trunk/dc/d88/tutorial_traincascade.html](http://docs.opencv.org/trunk/dc/d88/tutorial_traincascade.html)\n",
    "\n",
    "7. [http://www.pyimagesearch.com/2015/05/11/creating-a-face-detection-api-with-python-and-opencv-in-just-5-minutes/](http://www.pyimagesearch.com/2015/05/11/creating-a-face-detection-api-with-python-and-opencv-in-just-5-minutes/)\n",
    "\n",
    "8. [http://docs.opencv.org/2.4/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.html](http://docs.opencv.org/2.4/doc/tutorials/core/file_input_output_with_xml_yml/file_input_output_with_xml_yml.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}